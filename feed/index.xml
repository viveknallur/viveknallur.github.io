<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Vivek Nallur</title>
	<atom:link href="https://viveknallur.github.io/feed/" rel="self" type="application/rss+xml" />
	<link>https://viveknallur.github.io</link>
	<description>The site for Vivek&#039;s (mostly academic) life</description>
	<lastBuildDate>Thu, 28 Jun 2018 16:51:41 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.6</generator>
	<item>
		<title>Implementing Ethics in Machines</title>
		<link>https://viveknallur.github.io/implementing-ethics-in-machines/</link>
		<comments>https://viveknallur.github.io/implementing-ethics-in-machines/#respond</comments>
		<pubDate>Thu, 28 Jun 2018 16:49:14 +0000</pubDate>
		<dc:creator><![CDATA[Vivek Nallur]]></dc:creator>
				<category><![CDATA[Machine Ethics]]></category>

		<guid isPermaLink="false">https://viveknallur.github.io/?p=270</guid>
		<description><![CDATA[If we wanted to implement ethics in a robot, how would we do it? What ethics should we implement? Are Asimov&#8217;s three laws enough? Although, there seems to be philosophical consensus that Asimov&#8217;s Three Laws of Robotics are nowhere near enough for a satisfactory ethical robot, there are still attempts at creating robots that do [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>If we wanted to implement ethics in a robot, how would we do it? What ethics should we implement? Are Asimov&#8217;s three laws enough?</p>
<p>Although, there seems to be philosophical consensus that Asimov&#8217;s Three Laws of Robotics are nowhere near enough for a satisfactory ethical robot, there are still attempts at creating robots that do implement some version of Asimov&#8217;s Laws. Perhaps this tells us something about ourselves, as human beings. Even for researchers, fiction exerts a power of imagination and framing that is difficult to shake off.</p>
<p><span id="more-270"></span> How do we convince others (or even ourselves) that we&#8217;ve created an ethical machine? Is it better to create machines that have some understanding of the philosophical principle, and can therefore <em>reason</em> about why it should or should not perform an action? Or should we insist on machines that will <em>never</em> misbehave? The answers to these questions drive <span style="text-decoration: underline;">how</span> we implement ethics. Regardless of the actual ethical principle involved, our preference for reasoning vis-a-vis safety will influence how ethics are actually embedded in the machine.</p>
<h4>Logic  and Model Checking</h4>
<p>An interesting approach to implementing ethical principles into robots is the HERA approach<span id="42aQ7zKGizuj8xPCmOSFd" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1352741480&quot;]" data-footnote="undefined"><sup>1</sup></span>. HERA stands for <em>Hybrid Ethical Reasoning Agents</em> and assumes that there is no right ethical theory to implement. Rather, they implement multiple moral theories, which are modelled using logical formulae. The formulae are then evaluated for their `truth&#8217;, given the consequences that arise from actions that the robot could potentially take. If a particular formula resolves to true for a certain action, then the robot is able to conclude that that action is allowed by the ethical principle implemented by that formula. This is interesting because, rather than being tied to a single ethical stance, the robot is able to evaluate the same action from multiple ethical principles and allow humans to (potentially) pick which principle to prioritize. Actions and consequences are modelled using directed acyclic graphs in a causal agency model, which are then checked using a model checker.</p>
<h4>Cognition</h4>
<p>Instead of using logic or symbolic reasoning, Vanderelst and Winfield propose a cognition based approach to implementing ethics<span id="tcFeReGZpoGzPtHuU087m" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1009922757&quot;]" data-footnote="undefined"><sup>2</sup></span>. The cognition based approach is based on some amount of evidence that considers simulation to be a key factor in &#8216;thinking&#8217;<span id="8eLDgrKkGGYNKb1xB_Smz" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1027811230&quot;]" data-footnote="undefined"><sup>3</sup></span>. That is, it appears that that functions like <em>behaviour</em>, <em>perception</em>, and <em>anticipation</em> are made possible for human beings, due to the presence of structures in the brain that can simulate interaction with the outside world. Effectively, what simulation theory says is that thinking is the act of simulating interactions with the external environment, without actually having overt actions. Vanderelst and Winfield propose that this can be achieved in robots through the use of a sophisticated simulation module. Most robots follow a three-layered control architecture<span id="FFz014o5c1eZTivOollVY" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;795711616&quot;]" data-footnote="undefined"><sup>4</sup></span>, with each layer acting at different time-scales and different levels of abstraction. This is enhanced with the use of one more layer called the Ethical layer. The ethical layer consists of two modules: simulation module and the evaluation module. For each possible behaviour that the robot can do, the simulation module sends a prediction of the robot and the external environment&#8217;s states to the evaluation module. The evaluation module then assigns a value to the combination of the robot&#8217;s predicted state and the world&#8217;s predicted state. Based on this value, the robot chooses to either behave in a particular manner or not.</p>
<p>&nbsp;</p>
<div id="abt-bibliography" class="abt-bibliography noselect mceNonEditable" data-reflist="[&quot;1352741480&quot;,&quot;1009922757&quot;,&quot;1027811230&quot;,&quot;795711616&quot;]">
<div id="abt-bibliography__container" class="abt-bibliography__container">
<div id="1352741480">
<div class="csl-entry flush">
<div class="csl-left-margin">1.</div>
<div class="csl-right-inline">Lindner F, Bentzen MM, Nebel B. The HERA approach to morally competent robots. In: <i>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>. IEEE; 2017. doi:<a href="https://doi.org/10.1109/iros.2017.8206625" target="_blank" rel="noopener noreferrer">10.1109/iros.2017.8206625</a></div>
</div>
</div>
<div id="1009922757">
<div class="csl-entry flush">
<div class="csl-left-margin">2.</div>
<div class="csl-right-inline">Vanderelst D, Winfield A. An architecture for ethical robots inspired by the simulation theory of cognition. <i>C</i>. 2018;48:56-66. doi:<a href="https://doi.org/10.1016/j.cogsys.2017.04.002" target="_blank" rel="noopener noreferrer">10.1016/j.cogsys.2017.04.002</a></div>
</div>
</div>
<div id="1027811230">
<div class="csl-entry flush">
<div class="csl-left-margin">3.</div>
<div class="csl-right-inline">Hesslow G. The current status of the simulation theory of cognition. <i>B</i>. 2012;1428:71-79. doi:<a href="https://doi.org/10.1016/j.brainres.2011.06.026" target="_blank" rel="noopener noreferrer">10.1016/j.brainres.2011.06.026</a></div>
</div>
</div>
<div id="795711616">
<div class="csl-entry flush">
<div class="csl-left-margin">4.</div>
<div class="csl-right-inline">Kortenkamp D, Simmons R, Brugali D. Robotic Systems Architectures and Programming. In: <i>Springer Handbook of Robotics</i>. Springer International Publishing; 2016:283-306. doi:<a href="https://doi.org/10.1007/978-3-319-32552-1_12" target="_blank" rel="noopener noreferrer">10.1007/978-3-319-32552-1_12</a></div>
</div>
</div>
</div>
</div>
]]></content:encoded>
			<wfw:commentRss>https://viveknallur.github.io/implementing-ethics-in-machines/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Can Machines be Moral Agents?</title>
		<link>https://viveknallur.github.io/can-machines-be-moral-agents/</link>
		<comments>https://viveknallur.github.io/can-machines-be-moral-agents/#respond</comments>
		<pubDate>Tue, 12 Jun 2018 17:51:05 +0000</pubDate>
		<dc:creator><![CDATA[Vivek Nallur]]></dc:creator>
				<category><![CDATA[Machine Ethics]]></category>

		<guid isPermaLink="false">https://viveknallur.github.io/?p=261</guid>
		<description><![CDATA[The question of whether a machine can ever be a moral agent, i.e., be an agent that can be considered morally responsible, is dependent on which side of the philosophical debate one falls on. Normally, we consider human beings to be moral agents, but technology created by humans to be morally neutral. That is, a [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>The question of whether a machine can ever be a moral agent, i.e., be an agent that can be considered morally responsible, is dependent on which side of the philosophical debate one falls on. Normally, we consider human beings to be moral agents, but technology created by humans to be morally neutral. That is, a knife is not a moral entity and no one expects to reprimand the knife if it cuts one&#8217;s hand or praise it if it slices through vegetables. The human wielding the knife is usually responsible for the actions done, using the knife. Thus, the knife&#8217;s effect on the world, whether positive or negative, is not the responsibility of the knife, even though it is clearly the consequence of the knife. Does this mean that only human beings are morally responsible, for any consequence of technological objects in the environment? And are any artificial artifacts exempt from morality? Clearly, this is not entirely black-and-white. For instance, infants, and children are usually not expected to be morally responsible for their actions, while artificial persons (such as corporations) are. So, the natural-ness or artificiality is not the sole determining criteria.</p>
<p><span id="more-261"></span></p>
<p>Regardless of which ethical theory one believes in, most philosophers seem to agree that there are three properties to be examined while determining if an agent is morally responsible or not:</p>
<ol>
<li>Autonomy &#8211; This is the least contentious of all. If an act is forced, through design/coercion/any other means, then the agent cannot be held morally responsible for the act. This is also sometimes characterized as <em>free-will</em>. The agent must be able to choose among a multiplicity of actions (even &#8220;not acting&#8221; is an action) in a given situation, based on some internal states.</li>
<li>Intentionality &#8211; The agent must <span style="text-decoration: underline;">intend</span> to perform the action. Thus, sneezing/reflex actions are not moral actions, even if they affect the environment, positively or negatively. This is the most difficult to show/prove.</li>
<li>Responsibility &#8211; Is the agent in a position of responsibility or can the agent be held responsible for the action? (This is why small children are not considered to be full moral agents. They are not thought of as being responsible for the consequences of their actions)</li>
</ol>
<p>Deborah Johnson considers other conditions<span id="qiP_1LvH0ts6jaWrbszaC" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;3495340121&quot;]" data-footnote="undefined"><sup>1</sup></span> that can be used to decide whether an action is morally evaluate-able, but concedes that most of those conditions can be met for machines (or at least, met as convincingly as human beings) apart from <em>intending to act</em>.</p>
<h4>Autonomy</h4>
<p>Human beings like to think of themselves as autonomous, and machines and robots as technological artifacts having no autonomy because these are (ultimately) deterministic programs. Despite recent advances in Machine Learning and AI  where mechanisms have non-deterministic outputs, researchers such as Bringsjord contend that these are not truly autonomous actions, rather actions that are determined by a random factor<span id="1LPEdCPyOo9Rl7e1FfkOU" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1091718972&quot;]" data-footnote="undefined"><sup>2</sup></span>. The problem with this line of reasoning is that it applies to human beings as well. We are all products of our environment, our culture, our education, etc. So who is truly autonomous?</p>
<h4>Intentionality</h4>
<p>Intentionality is, to my mind, the most difficult aspect of the triad. In theory, for most machines/robots/artificial agents, we could have access to their internal states and thereby (posthoc) show that their actions were intentional. However, sufficiently complex internal states along with autonomy would make showing this rigorously, impossible. Again, this argument about non-rigour holds for human beings as well. According to Sullins, this rigour is really not necessary<span id="iSXrXIdle3dSnfGPkED2R" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;4176879265&quot;]" data-footnote="undefined"><sup>3</sup></span>. Given complex behaviour, when one is forced to rely on what <em>seems to</em> <em>people</em> like pre-disposition or intention, it is enough to call this intentionality. Deborah Johnson, on the other hand, contends that there is very definite intentionality: intentionality of designers and intentionality of users<span id="~j75Q5mj7GUR3pwFANisY" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;3495340121&quot;]" data-footnote="undefined"><sup>1</sup></span>. She views the robot/autonomous agent as &#8220;<em>poised to behave</em>&#8221; in the way that the designers intended it to, but the &#8220;intentionality of computer systems is inert or latent without the intentionality of the user&#8221;.  So, though intentionality on the part of the agent is difficult to prove, many researchers are prepared to concede that some sort of intentionality could exist.</p>
<h4>Responsibility</h4>
<p>If a machine&#8217;s behaviour can only reasonably be described if it has a sense of responsibility, <em>i.e.</em>, the behaviour only makes sense if the machine (in the absence of any known malfunction) had a sense of responsibility, then we must ascribe responsibility to the machine<span id="qS_KxoIA~MEqq6Kvxf2bv" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;4176879265&quot;]" data-footnote="undefined"><sup>3</sup></span>. This effectively means that:</p>
<ul>
<li>if the machine knows that its actions will change the environment, and then autonomically decides on which action to take, then it must &#8216;want&#8217; that change.</li>
<li>If it &#8216;wants&#8217; this change, and takes action to effect that change, then it is effectively assuming that it is responsible for bringing about that change.</li>
</ul>
<p>There are two other interesting positions, on whether an agent is a moral agent or not. Nadeau contends that only a fully thought out action with reasoning can be a free action. And if free will is a necessity for moral agency, then human beings are <em>not moral agents!</em> In fact, only a robot/agent can be completely logical and therefore completely free. Hence, if we can build such a machine, it will be the first truly moral agent!<span id="CCjOuGGoQI2etswcWcXtW" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1077859105&quot;]" data-footnote="undefined"><sup>4</sup></span></p>
<p>The other interesting position comes from Luciano Floridi<span id="oLcs_6BBU47aBsEPzLOCG" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;125313493&quot;]" data-footnote="undefined"><sup>5</sup></span>. He argues that a theory-of-mind based morality is unnecessary, and that morality can be discussed without recourse to anthropomorphically charged and philosophically contested concepts. According to Floridi, the entire notion of agent-hood depends on the level of abstraction from which we view a system. That is, even human beings would not be considered agents, if we viewed human beings as collections of molecules subject to bio-chemical and physical forces. The bio-chemical perspective is technically correct, but unsuitable for a discussion of society, laws or ethics. To discuss such concepts relating to agent-hood, we typically use a different level of abstraction. Hence, it is enough for the machine to be subjected to the same level of abstraction that a human is, while deciding whether the machine is an agent or not. He argues that the right level of abstraction must contain three criteria: (a) <em>interactivity; </em>(b) <em>autonomy; </em> and (c) <em>adaptability</em>. He then goes on to discuss multiple different machines, that could potentially satisfy each or any of these criteria. He finally posits that any machine that satisfies all three criteria has as much right to agenthood, as a human being.</p>
<p>In between the no-agenthood position and the full-agenthood position, there are other positions to take. Johnson, for instance, takes the position that while machines can never be full-agents, they are quite definitely moral entities<span id="MGLgIOXSxc3ZY3d2ZNPtY" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;3495340121&quot;]"><sup>1</sup></span>.</p>
<div id="abt-bibliography" class="abt-bibliography noselect mceNonEditable" data-reflist="[&quot;3495340121&quot;,&quot;1091718972&quot;,&quot;4176879265&quot;,&quot;1077859105&quot;,&quot;125313493&quot;]">
<div id="abt-bibliography__container" class="abt-bibliography__container">
<div id="3495340121">
<div class="csl-entry flush">
<div class="csl-left-margin">1.</div>
<div class="csl-right-inline">Johnson DG. Computer systems: Moral entities but not moral agents. <i>E</i>. 2006;8(4):195-204. doi:<a href="https://doi.org/10.1007/s10676-006-9111-5" target="_blank" rel="noopener noreferrer">10.1007/s10676-006-9111-5</a></div>
</div>
</div>
<div id="1091718972">
<div class="csl-entry flush">
<div class="csl-left-margin">2.</div>
<div class="csl-right-inline">Bringsjord S. Ethical robots: the future can heed us. <i>A</i>. 2007;22(4):539-550. doi:<a href="https://doi.org/10.1007/s00146-007-0090-9" target="_blank" rel="noopener noreferrer">10.1007/s00146-007-0090-9</a></div>
</div>
</div>
<div id="4176879265">
<div class="csl-entry flush">
<div class="csl-left-margin">3.</div>
<div class="csl-right-inline">Sullins JP. When Is a Robot a Moral Agent? In: Anderson M, Anderson SL, eds. <i>Machine Ethics</i>. Cambridge University Press; 0:151-161. doi:<a href="https://doi.org/10.1017/cbo9780511978036.013" target="_blank" rel="noopener noreferrer">10.1017/cbo9780511978036.013</a></div>
</div>
</div>
<div id="1077859105">
<div class="csl-entry flush">
<div class="csl-left-margin">4.</div>
<div class="csl-right-inline">Nadeau JE. Only Androids Can Be Ethical. <i>Thinking about Android Epistemology</i>. 2006:241&#8211;248.</div>
</div>
</div>
<div id="125313493">
<div class="csl-entry flush">
<div class="csl-left-margin">5.</div>
<div class="csl-right-inline">Floridi L. On the Morality of Artificial Agents. In: Anderson M, Anderson SL, eds. <i>Machine Ethics</i>. Cambridge University Press; 0:184-212. doi:<a href="https://doi.org/10.1017/cbo9780511978036.016" target="_blank" rel="noopener noreferrer">10.1017/cbo9780511978036.016</a></div>
</div>
</div>
</div>
</div>
]]></content:encoded>
			<wfw:commentRss>https://viveknallur.github.io/can-machines-be-moral-agents/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>What is Machine Ethics?</title>
		<link>https://viveknallur.github.io/what-is-machine-ethics/</link>
		<comments>https://viveknallur.github.io/what-is-machine-ethics/#respond</comments>
		<pubDate>Tue, 22 May 2018 22:35:43 +0000</pubDate>
		<dc:creator><![CDATA[Vivek Nallur]]></dc:creator>
				<category><![CDATA[Machine Ethics]]></category>

		<guid isPermaLink="false">https://viveknallur.github.io/?p=232</guid>
		<description><![CDATA[What does it mean for a machine to have ethics? Is that even possible? Why would we care if a machine is ethical or not, as long as it does what we tell it to do? There are a variety of positions on each of these questions. The two extreme positions on whether a machine [&#8230;]]]></description>
				<content:encoded><![CDATA[<p><em>What does it mean for a machine to have ethics? Is that even possible? Why would we care if a machine is ethical or not, as long as it does what we tell it to do?</em></p>
<p>There are a variety of positions on each of these questions. The two extreme positions on whether a machine can be ethical are as follows:</p>
<ul>
<li><em>Machines can never have ethics, since machines are bereft of emotions and emotions are integral to ethics</em>.</li>
<li><em>Humans are not special. It is entirely possible that some machines could have a sense of ethics.</em></li>
</ul>
<p><span id="more-232"></span></p>
<p>The first position essentially contends that there is something special about human beings which gives us a sense of ethics. According to this position, human beings have free-will, intentionality and consciousness, all of which are pre-requisites for ethics. If you agree with this position, then there is no more debate to be had. Since machines, even autonomous ones do not have <span style="text-decoration: underline;">intentionality</span> or <span style="text-decoration: underline;">consciousness</span>, they can never be said to have any sense of ethics. The trouble with this position is these foundational terms are not well-defined. <em>What exactly is consciousness? </em>Even those in the I-know-it-when-I-see-it camp can only possibly know that they themselves are conscious, but can never really tell if another human being is conscious.</p>
<p>The second position contends that it is irrelevant whether a machine is human-like or not, but it may still have the ability to be ethical and can be evaluated on that question. James H. Moor in his <em>The Nature, Importance and Difficulty of Machine Ethics</em><span id="CetfJsfrpQCEU7~PTXvdc" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1888356253&quot;]" data-footnote="undefined"><sup>1</sup></span> contends that there are four different degrees of ethical evaluations for autonomous agents:</p>
<ol>
<li>Ethical Impact Agents &#8211; Autonomous machines or robots or software that has an ethical impact on the world it interacts with. He uses the example of usage robot jockeys for camel racing in Qatar. Having robot jockeys decreases the demand for children being trafficked from poorer countries to be camel jockeys, thus having an ethical impact.</li>
<li>Implicit Ethical Agents &#8211; This refers to software / hardware this is created to follow some ethical principles, but does not actually reason about ethics itself. Thus, any banking software that ensures that banking rules are followed is an implicit ethical agent. It is implicitly ethical because the creators of the software programmed it to behave in an ethical manner, and not cheat people out of money. However, the software itself cannot reason about its behaviour.</li>
<li>Explicit Ethical Agents &#8211; These are software / hardware that have some ethical principles programmed into them, and the agents use these principles to choose from several alternative actions, autonomously. For example, Michael Anderson, Susan Anderson and Chris Armen discuss <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.475.7355&amp;rep=rep1&amp;type=pdf">two ethical theories that they programmed into a robot,</a> but it (the robot) is far from being called an ethical agent. They contend that regardless of the efficacy (completeness, soundness, robustness) of the theory, it could be better to have ethical principles programmed into robots than not. This is so because machines can at least be consistent, which is lacking in human beings.</li>
<li>Full Ethical Agents &#8211; Adult human beings are usually considered full ethical agents, because not only do they (presumably) act ethically, but they can also provide a justification for why they acted the way they did. This is usually the most contentious part of machines with ethics. Some people believe that only full ethical agents can be called ethical agents at all.</li>
</ol>
<p>Whether we consider machines to have consciousness, free-will and intentionality or not, we will have to concede that as machines become more and more autonomous, they need to have some mechanism to ensure that they behave ethically towards human beings. Nick Bostrom created his now-famous example of a super-intelligence that consumes all of the planet&#8217;s resources trying to make more and more paperclips. Given that we may not be able to thwart such a super-intelligence, it might be only ethical considerations that prevent the super-intelligence from drowning the planet in paperclips.</p>
<p>&nbsp;</p>
<div id="abt-bibliography" class="abt-bibliography noselect mceNonEditable" data-reflist="[&quot;1888356253&quot;]">
<div id="abt-bibliography__container" class="abt-bibliography__container">
<div id="1888356253">
<div class="csl-entry flush">
<div class="csl-left-margin">1.</div>
<div class="csl-right-inline">Moor JH. The Nature, Importance, and Difficulty of Machine Ethics. <i>I</i>. 2006;21(4):18-21. doi:<a href="https://doi.org/10.1109/mis.2006.80" target="_blank" rel="noopener noreferrer">10.1109/mis.2006.80</a></div>
</div>
</div>
</div>
</div>
]]></content:encoded>
			<wfw:commentRss>https://viveknallur.github.io/what-is-machine-ethics/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Machine Ethics &#8211; Welcome</title>
		<link>https://viveknallur.github.io/machine-ethics-welcome/</link>
		<pubDate>Tue, 24 Apr 2018 15:01:44 +0000</pubDate>
		<dc:creator><![CDATA[Vivek Nallur]]></dc:creator>
				<category><![CDATA[Machine Ethics]]></category>

		<guid isPermaLink="false">https://viveknallur.github.io/?p=129</guid>
		<description><![CDATA[This is the first post on the Machine Ethics blog. In this blog, I and some guests, will be posting article-reviews, positions, thoughts, and (sometimes) news about the field of Machine Ethics.  These may relate to the field of Artificial Intelligence, Philosophy, Logic, Law, Politics or (better yet) all of them!! If you want to [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>This is the first post on the Machine Ethics blog. In this blog, I and some guests, will be posting article-reviews, positions, thoughts, and (sometimes) news about the field of Machine Ethics.  These may relate to the field of Artificial Intelligence, Philosophy, Logic, Law, Politics or (better yet) all of them!!</p>
<p>If you want to contribute a post, <a href="https://viveknallur.github.io/contact/contact-me/">get in touch</a>.</p>
]]></content:encoded>
			</item>
		<item>
		<title>Awarded Seed Funding</title>
		<link>https://viveknallur.github.io/awarded-seed-funding/</link>
		<comments>https://viveknallur.github.io/awarded-seed-funding/#respond</comments>
		<pubDate>Mon, 23 Apr 2018 10:29:22 +0000</pubDate>
		<dc:creator><![CDATA[Vivek Nallur]]></dc:creator>
				<category><![CDATA[Vivek's Research Blog]]></category>

		<guid isPermaLink="false">https://viveknallur.github.io/?p=237</guid>
		<description><![CDATA[I&#8217;ve just been awarded Seed Funding to start creating an inter-disciplinary network on the theme of Inserting Ethics Into Autonomous Machines.  Yay! The idea is (initially) to have invited talks from multiple disciplines (e.g. law, philosophy, robotics, hci) to flesh out various aspects of ethics from an implementation perspective. What sort of ethical theories can/should be [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>I&#8217;ve just been awarded Seed Funding to start creating an inter-disciplinary network on the theme of <em>Inserting Ethics Into Autonomous Machines</em>.  Yay!</p>
<p>The idea is (initially) to have invited talks from multiple disciplines (e.g. law, philosophy, robotics, hci) to flesh out various aspects of ethics from an implementation perspective. What sort of ethical theories can/should be considered? How should they be implemented? How could we verify that a machine that claims to follow an ethical principle, is indeed ethical?</p>
<p>The School of Computer Science has kindly agreed to co-fund some of these talks, which means that we should be able to reimburse speakers from the EU, for flights as well as a night&#8217;s stay. Looking forward to an eclectic and thought-provoking set of talks.</p>
<p>If you have any ideas for speakers or topics, <a href="https://viveknallur.github.io/contact-me/">get in touch</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://viveknallur.github.io/awarded-seed-funding/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Eamonn O’Toole submits his thesis on decentralized emergence detection!</title>
		<link>https://viveknallur.github.io/eamonn-otoole-submits-his-thesis-on-decentralized-emergence-detection/</link>
		<comments>https://viveknallur.github.io/eamonn-otoole-submits-his-thesis-on-decentralized-emergence-detection/#respond</comments>
		<pubDate>Sat, 07 Nov 2015 17:43:40 +0000</pubDate>
		<dc:creator><![CDATA[Vivek Nallur]]></dc:creator>
				<category><![CDATA[Vivek's Research Blog]]></category>

		<guid isPermaLink="false">http://127.0.0.1/wordpress/?p=225</guid>
		<description><![CDATA[Eamonn has just submitted his thesis entitled “Decentralized Detection of Emergence in Complex Adaptive Systems”. Way to go, Eamonn! Very well done! Of course, it’s really special to me since he’s my first PhD student mentee. It’s been a pleasure discussing philosophies, ideas, problems and solution concepts with you, mate. Hope you go far! The [&#8230;]]]></description>
				<content:encoded><![CDATA[<p>Eamonn has just submitted his thesis entitled “Decentralized Detection of Emergence in Complex Adaptive Systems”. Way to go, Eamonn! Very well done! Of course, it’s really special to me since he’s my first PhD student mentee. It’s been a pleasure discussing philosophies, ideas, problems and solution concepts with you, mate. Hope you go far!</p>
<p>The thesis looks at the problem of detecting emergence in a large-scale complex adaptive systems, where each part could potentially be adapting independently. Given the large-scale and decentralized nature of such systems, the big questions are:</p>
<ul>
<li><em>Is it possible to detect emergence in a decentralized fashion?</em></li>
<li><em>How do you know which variables are interesting, given that the environment and the components themselves, change dynamically?</em></li>
</ul>
<p>For answers to these questions (and more), read his thesis! Too busy to read a thesis (you&#8217;re missing out)? Check out our TAAS Paper<span id="PgnqkLFUHu44vtaqij3Ze" class="abt-citation noselect mceNonEditable" data-reflist="[&quot;1777792285&quot;]" data-footnote="undefined"><sup>1</sup></span>.</p>
<div id="abt-bibliography" class="abt-bibliography noselect mceNonEditable" data-reflist="[&quot;1777792285&quot;]">
<div id="abt-bibliography__container" class="abt-bibliography__container">
<div id="1777792285">
<div class="csl-entry flush">
<div class="csl-left-margin">1.</div>
<div class="csl-right-inline">O’toole E, Nallur V, Clarke S. Decentralised Detection of Emergence in Complex Adaptive Systems. <i>A</i>. 2017;12(1):1-31. doi:<a href="https://doi.org/10.1145/3019597" target="_blank" rel="noopener noreferrer">10.1145/3019597</a></div>
</div>
</div>
</div>
</div>
]]></content:encoded>
			<wfw:commentRss>https://viveknallur.github.io/eamonn-otoole-submits-his-thesis-on-decentralized-emergence-detection/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
